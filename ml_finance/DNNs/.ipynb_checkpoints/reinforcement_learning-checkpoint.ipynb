{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "486f091d-65e2-45d5-8797-6954ab4c307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinforcement learning \n",
    "# no need for large labels and features data to be given upfront\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pylab import plt, mpl\n",
    "import gym\n",
    "plt.style.use('seaborn')\n",
    "mpl.rcParams['savefig.dpi'] = 300\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "np.set_printoptions(precision = 4, suppress = True)\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88decd70-727b-4b65-9299-cbdddba74ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8feb0d3a-9995-43c4-b605-63576890cc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/gym/core.py:257: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  \"Function `env.seed(seed)` is marked as deprecated and will be removed in the future. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[100]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.seed(100)\n",
    "env.action_space.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f72bbd-5329-42b9-b41d-23c21c97eb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000e+00 -3.4028e+38 -4.1888e-01 -3.4028e+38], [4.8000e+00 3.4028e+38 4.1888e-01 3.4028e+38], (4,), float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88b2a2e4-3ef0-4ba5-9b74-14192dc6df92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.8  ,   inf, 0.419,   inf], dtype=float16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min and max values in the observation space\n",
    "env.observation_space.low.astype(np.float16)\n",
    "env.observation_space.high.astype(np.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e56c3b-8c42-4387-9357-c282b566e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86347e0a-6718-4749-ad83-ac3fe1ae95f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0335,  0.0097, -0.0211, -0.0457], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3df8c71-7628-4b6d-8cac-333d3bd60b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0085d1d6-dd8f-424d-b1eb-5c01e7beb79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "735943a1-02ef-45c0-ad37-ce405b981293",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57a29398-b423-48c8-8b0c-9ad56619f699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01c4c329-eca6-4f5d-8152-9f81422847cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "968953bd-6c7d-43ac-b2fe-5d54bc252274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0337,  0.2051, -0.022 , -0.345 ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4b2122d-ff84-41d9-968f-8910f5ed810b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efa6c00a-e033-4b1b-a169-73f690b79406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done   #done would be = True if the pole falls down "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06641ce4-0387-42dd-a0be-5f76a0ef380c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 1 | state=[ 0.0476  0.2043  0.0298 -0.2424] | action=1 | reward=1.0\n",
      "step= 2 | state=[0.0516 0.0088 0.025  0.0596] | action=0 | reward=1.0\n",
      "step= 3 | state=[ 0.0518  0.2036  0.0262 -0.2251] | action=1 | reward=1.0\n",
      "step= 4 | state=[0.0559 0.0081 0.0217 0.0757] | action=0 | reward=1.0\n",
      "step= 5 | state=[ 0.0561 -0.1874  0.0232  0.3752] | action=0 | reward=1.0\n",
      "step= 6 | state=[ 0.0523 -0.3828  0.0307  0.6751] | action=0 | reward=1.0\n",
      "step= 7 | state=[ 0.0447 -0.5783  0.0442  0.9773] | action=0 | reward=1.0\n",
      "step= 8 | state=[ 0.0331 -0.3838  0.0638  0.6988] | action=1 | reward=1.0\n",
      "step= 9 | state=[ 0.0254 -0.1896  0.0777  0.4269] | action=1 | reward=1.0\n",
      "step=10 | state=[0.0216 0.0043 0.0863 0.1596] | action=1 | reward=1.0\n",
      "step=11 | state=[ 0.0217  0.1981  0.0895 -0.1046] | action=1 | reward=1.0\n",
      "step=12 | state=[0.0257 0.0018 0.0874 0.2149] | action=0 | reward=1.0\n",
      "step=13 | state=[ 0.0257  0.1956  0.0917 -0.049 ] | action=1 | reward=1.0\n",
      "step=14 | state=[ 0.0296  0.3893  0.0907 -0.3114] | action=1 | reward=1.0\n",
      "step=15 | state=[ 0.0374  0.583   0.0845 -0.5742] | action=1 | reward=1.0\n",
      "step=16 | state=[ 0.0491  0.3868  0.073  -0.2561] | action=0 | reward=1.0\n",
      "step=17 | state=[ 0.0568  0.5808  0.0679 -0.5249] | action=1 | reward=1.0\n",
      "step=18 | state=[ 0.0684  0.7749  0.0574 -0.7955] | action=1 | reward=1.0\n",
      "step=19 | state=[ 0.0839  0.579   0.0414 -0.4853] | action=0 | reward=1.0\n",
      "step=20 | state=[ 0.0955  0.7736  0.0317 -0.7647] | action=1 | reward=1.0\n",
      "step=21 | state=[ 0.111   0.9682  0.0164 -1.0472] | action=1 | reward=1.0\n",
      "step=22 | state=[ 0.1303  1.1631 -0.0045 -1.3347] | action=1 | reward=1.0\n",
      "step=23 | state=[ 0.1536  0.9681 -0.0312 -1.0434] | action=0 | reward=1.0\n",
      "step=24 | state=[ 0.1729  0.7734 -0.0521 -0.7607] | action=0 | reward=1.0\n",
      "step=25 | state=[ 0.1884  0.9692 -0.0673 -1.0693] | action=1 | reward=1.0\n",
      "step=26 | state=[ 0.2078  0.775  -0.0887 -0.7984] | action=0 | reward=1.0\n",
      "step=27 | state=[ 0.2233  0.9712 -0.1046 -1.1176] | action=1 | reward=1.0\n",
      "step=28 | state=[ 0.2427  1.1675 -0.127  -1.4412] | action=1 | reward=1.0\n",
      "step=29 | state=[ 0.2661  1.364  -0.1558 -1.7707] | action=1 | reward=1.0\n",
      "step=30 | state=[ 0.2934  1.5605 -0.1912 -2.1075] | action=1 | reward=1.0\n",
      "step=31 | state=[ 0.3246  1.7569 -0.2334 -2.4527] | action=1 | reward=1.0\n",
      "*** FAILED ***\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for e in range(1,200):\n",
    "    a = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(a)\n",
    "    print(f'step={e:2d} | state={state} | action={a} | reward={reward}')\n",
    "    if done and (e+1)< 200:\n",
    "        print('*** FAILED ***')\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "596e5796-3329-42c7-87e1-b0f31b8e0752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20a08a1a-9788-4fc1-8ad0-509fc84f895f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0868, -0.4433, -0.151 ,  0.6896])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "weights = np.random.random(4) * 2 -1 \n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e62f85b-fced-45b3-bb2c-f0d3dec459d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ae08e8e-7066-4d75-a92c-29cdb2fe63be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0188, -0.031 ,  0.0481, -0.0215], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "280d0ed9-4027-4100-b122-b01b0822698d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0067370952515455294"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.dot(state, weights)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6555a999-e79c-4720-9126-4c1f01ae1c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if s <0: \n",
    "    a = 0\n",
    "else:\n",
    "    a = 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0a41b63-4ae8-4a24-bc0d-29273da16731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_episode(env,weights):\n",
    "    state = env.reset()\n",
    "    treward = 0\n",
    "    for _ in range(200):\n",
    "        s = np.dot(state, weights)\n",
    "        a = 0 if s<0 else 1\n",
    "        state, reward, done, info = env.step(a)\n",
    "        treward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return treward\n",
    "run_episode(env, weights)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c9033de-4fc7-45df-87f1-664bd5fe61b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# montecarlo simulation to test a large number of weights \n",
    "\n",
    "def set_seeds(seed = 100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    env.seed(seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df38a404-a72f-42b2-98b7-7a73ff1b6b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE | episode = 1\n",
      "SUCCESS | episode = 2\n"
     ]
    }
   ],
   "source": [
    "set_seeds()\n",
    "num_episodes = 1000\n",
    "\n",
    "besttreward = 0\n",
    "for e in range(1,num_episodes + 1):\n",
    "    weights = np.random.rand(4)*2 + 1 \n",
    "    treward = run_episode(env,weights)\n",
    "    if treward > besttreward:\n",
    "        besttreward = treward\n",
    "        bestweights = weights\n",
    "    if treward == 200:\n",
    "        print(f'SUCCESS | episode = {e}')\n",
    "        break\n",
    "    print(f'UPDATE | episode = {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecb8563a-91b6-44a1-ac40-2db0a2c9f0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0094, 1.2431, 2.3415, 2.6517])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c93be6f-b0d5-442a-bc94-08b1165dbcef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "for _ in range(100):\n",
    "    treward = run_episode(env,weights)\n",
    "    res.append(treward)\n",
    "res[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44cb7fa2-e193-4ef5-8325-85041540903f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/tensorflow_core/python/framework/dtypes.py:597: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object,\n",
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/tensorflow_core/python/framework/dtypes.py:605: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool,\n",
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:106: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object:\n",
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool:\n",
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n",
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:568: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  (np.object, string),\n",
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  (np.bool, bool),\n",
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:100: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object: SlowAppendObjectArrayToTensorProto,\n",
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:101: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool: SlowAppendBoolArrayToTensorProto,\n",
      "Using TensorFlow backend.\n",
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/keras/callbacks/callbacks.py:19: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "# Neural Network Agent\n",
    "import tensorflow as tf \n",
    "from keras.layers import Dense, Dropout \n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import adam, RMSprop\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e519b081-36b0-4f79-87b9-a43443ab6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed = 100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b4c1138-aaff-43f4-a80c-19b763f5b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploration and exploitation \n",
    "\n",
    "class NNAgent:\n",
    "    def __init__(self):\n",
    "        self.max = 0\n",
    "        self.scores = list()\n",
    "        self.memory = list()\n",
    "        self.model = self._build_model()\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim = 4, activation = 'relu'))\n",
    "        model.add(Dense(1,activation = 'sigmoid'))\n",
    "        model.compile(loss = 'binary_crossentropy', \n",
    "                      optimizer = RMSprop(lr = 0.001))\n",
    "        return model\n",
    "    def act(self,state):\n",
    "        if random.random() <0.5:\n",
    "            return env.action_space.sample()\n",
    "        action = np.where(self.model.predict(\n",
    "            state, batch_size = None)[0,0] > 0.5, 1, 0)\n",
    "        return action\n",
    "    def train_model(self, state, action):\n",
    "        self.model.fit(state,np.array([action,]), \n",
    "                      epochs = 1, verbose = False)\n",
    "    def learn(self, episodes):\n",
    "        for e in range(1,episodes+1):\n",
    "            state = env.reset()\n",
    "            for _ in range(201):\n",
    "                state = np.reshape(state,[1,4])\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                if done:\n",
    "                    score = _ + 1\n",
    "                    self.scores.append(score)\n",
    "                    self.max = max(score, self.max)\n",
    "                    break\n",
    "                self.memory.append((state,action))\n",
    "                self.train_model(state,action)\n",
    "                state = next_state\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f9e8fdc-7c30-4688-b539-d95c0d6f677f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/gym/core.py:257: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  \"Function `env.seed(seed)` is marked as deprecated and will be removed in the future. \"\n",
      "2023-01-26 10:08:18.269151: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-26 10:08:18.270060: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n",
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/keras/engine/training_utils.py:811: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  if isinstance(loss, collections.Mapping):\n"
     ]
    }
   ],
   "source": [
    "set_seeds(100)\n",
    "agent = NNAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebaa9a5f-80ca-4c7e-8cff-da403dc4f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11ecc482-ba29-433e-a0ab-8eebf8de071f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ananth/miniconda3/envs/tensorflow_env/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:339: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  if not isinstance(values, collections.Sequence):\n"
     ]
    }
   ],
   "source": [
    "agent.learn(episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77a01f3b-0bcc-49fe-a2e5-af8443b75d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.296"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(agent.scores)/ len(agent.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15b1bc2d-26b5-4269-a750-4de481b5830b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0335,  0.0097, -0.0211, -0.0457],\n",
       "       [ 0.0337,  0.2051, -0.022 , -0.345 ],\n",
       "       [ 0.0378,  0.4005, -0.0289, -0.6445],\n",
       "       ...,\n",
       "       [ 0.0992,  0.3943, -0.1196, -0.8131],\n",
       "       [ 0.1071,  0.5908, -0.1358, -1.1408],\n",
       "       [ 0.1189,  0.7874, -0.1587, -1.4728]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = np.array([m[0][0] for m in agent.memory])\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf39156c-6a87-49d6-8b5b-6832d9f391e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = np.array([m[1] for m in agent.memory])\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b51dd9ff-7678-4434-ab7a-efda3eded297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7612231620039037"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.where(agent.model.predict(f)>0.5, 1, 0), l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05410723-d35a-48df-bb85-982bbda3ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dql agent - q learning \n",
    "# ql learning takes into account immediate rewards and delayed rewards \n",
    "\n",
    "from collections import deque \n",
    "from keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15dcb0de-ea3b-4774-af53-c996da503214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent:\n",
    "    def __init__(self, gamma = 0.95, hu = 24, opt = Adam, \n",
    "                lr = 0.001, finish = False):\n",
    "        self.finish = finish\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = 32\n",
    "        self.max_treward = 0\n",
    "        self.averages = list()\n",
    "        self.memory = deque(maxlen = 2000)\n",
    "        self.osn = env.observation_space.shape[0]\n",
    "        self.model = self._build_model(hu,opt,lr)\n",
    "    def _build_model(self,hu,opt,lr):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hu,input_dim = self.osn, \n",
    "                       activation = 'relu'))\n",
    "        model.add(Dense(hu,activation = 'relu'))\n",
    "        model.add(Dense(env.action_space.n, activation = 'linear'))\n",
    "        model.compile(loss = 'mse', optimizer = opt(lr = lr))\n",
    "        return model\n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        action = self.model.predict(state)[0]\n",
    "        return np.argmax(action)\n",
    "    def replay(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            if not done:\n",
    "                reward += self.gamma * np.max(\n",
    "                    self.model.predict(next_state)[0])\n",
    "                target = self.model.predict(state)\n",
    "                target[0,action] = reward\n",
    "                self.model.fit(state, target, epochs=1, \n",
    "                              verbose = False)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def learn(self,episodes):\n",
    "        trewards = []\n",
    "        for e in range(1,episodes+1):\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, self.osn])\n",
    "            for _ in range(5000):\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state = np.reshape(next_state, \n",
    "                                        [1, self.osn])\n",
    "                self.memory.append([state, action, reward, \n",
    "                                   next_state, done])\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    treward = _ + 1 \n",
    "                    trewards.append(treward)\n",
    "                    av = sum(trewards[-25:])/25\n",
    "                    self.averages.append(av)\n",
    "                    self.max_treward = max(self.max_treward, treward)\n",
    "                    templ = 'episode: {:4d}/{} | treward: {:4d} | '\n",
    "                \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97658477-3f7a-4c9e-b53e-678fb0749989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
